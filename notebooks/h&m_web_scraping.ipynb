{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promotional-parameter",
   "metadata": {},
   "source": [
    "# H & M --- Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-semiconductor",
   "metadata": {},
   "source": [
    "## LIBRARIES AND SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outside-symposium",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T18:20:27.073757Z",
     "start_time": "2022-06-26T18:20:27.061752Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "judicial-arctic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T18:20:27.900225Z",
     "start_time": "2022-06-26T18:20:27.892219Z"
    }
   },
   "outputs": [],
   "source": [
    "# simulates a browser\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebkit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "\n",
    "# home page\n",
    "url = 'https://www2.hm.com/en_us/men/products/jeans.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-underwear",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. COLLECT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-plymouth",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.1. Extraction the number of items on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moral-craft",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-25T22:13:22.575413Z",
     "start_time": "2022-06-25T22:13:19.315210Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API request\n",
    "page = requests.get(url, headers = headers)\n",
    "\n",
    "# transform the html request into a beautiful soup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# page size\n",
    "itens = soup.find_all('h2')[2]\n",
    "\n",
    "itens_shown = int(itens['data-items-shown'])\n",
    "total_itens = int(itens['data-total'])\n",
    "\n",
    "page_size = str(int(np.ceil(total_itens / itens_shown) * itens_shown))\n",
    "\n",
    "# new url with the total amount of items\n",
    "new_url = 'https://www2.hm.com/en_us/men/products/jeans.html?sort=stock&image-size=small&image=model&offset=0&page-size=' + page_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-rachel",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.2. Extraction of links for each product on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acoustic-algorithm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-25T22:13:25.833769Z",
     "start_time": "2022-06-25T22:13:22.575413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API request\n",
    "page = requests.get(new_url, headers = headers)\n",
    "\n",
    "# transform the html request into a beautiful soup object\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# identifies the products\n",
    "product_item_li = soup.find_all('li', class_ = 'product-item')\n",
    "\n",
    "# extract the details url\n",
    "domain = 'https://www2.hm.com'\n",
    "url = [domain + i.find('a')['href'] for i in product_item_li]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-tulsa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Exploring the H&M website, it was possible to see that each different color for a particular pair of pants had its own characteristics. For this reason, a routine to extract all the links of each color available for each product was made, in order to obtain the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "gothic-chosen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-25T22:15:44.872920Z",
     "start_time": "2022-06-25T22:13:25.833769Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# creates the dataframe structure\n",
    "color_info = pd.DataFrame(columns = ['color_url', 'color_id', 'color_name'])\n",
    "\n",
    "for u in url:\n",
    "    # API request\n",
    "    page = requests.get(u, headers = headers)\n",
    "\n",
    "    # transform the html request into a beautiful soup object\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # collects information about colors\n",
    "    info = soup.find_all('a', class_ = 'filter-option')\n",
    "    \n",
    "    # auxiliary dataframe\n",
    "    aux = pd.DataFrame(columns = ['color_url', 'color_id', 'color_name'])\n",
    "    \n",
    "    aux['color_url']  = [domain + i['href'] for i in info]\n",
    "    aux['color_name'] = [i['title'] for i in info]\n",
    "    aux['color_id']   = [i['data-articlecode'] for i in info]\n",
    "    \n",
    "    # contacts collected informations in a single dataframe\n",
    "    color_info = pd.concat([color_info, aux], ignore_index = True)\n",
    "\n",
    "# drop duplicates\n",
    "color_info = color_info.drop_duplicates(keep = 'first').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-nancy",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.3. Extract information from each product "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forced-franchise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-25T22:19:21.075824Z",
     "start_time": "2022-06-25T22:15:44.872920Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# empty dataframe\n",
    "df_details = pd.DataFrame()\n",
    "\n",
    "cols = ['Art. No.', 'Composition', 'Fit', 'Product safety', 'Size']\n",
    "df_pattern = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for url in color_info.loc[:, 'color_url']:\n",
    "    # API request\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    # transform the html request into a beautiful soup object\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "     \n",
    "    # price and name\n",
    "    info1 = soup.find('div', class_ = 'inner')\n",
    "    name = info1.find('h1').text\n",
    "    price = info1.find('span', class_ = 'price-value').text\n",
    "    \n",
    "    # product features\n",
    "    info2 = soup.find('div', class_ = 'details parbase')\n",
    "    aux2 = [list(filter(None, i.get_text().split('\\n'))) for i in info2.find('dl').find_all('div')]\n",
    "    \n",
    "    # index\n",
    "    line = color_info[color_info['color_url'] == url].index\n",
    "    \n",
    "    # put the data in the dataframe color_info\n",
    "    color_info.loc[line, 'name']    = name\n",
    "    color_info.loc[line, 'price']   = price\n",
    "    color_info['web_scraping_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # reaname dataframe\n",
    "    aux2 = pd.DataFrame(aux2).T\n",
    "    aux2.columns = aux2.iloc[0]\n",
    "    \n",
    "    # delete first row\n",
    "    aux2 = aux2.iloc[1:].fillna(method = 'ffill')\n",
    "    \n",
    "    # garantee the same number of columns\n",
    "    aux2 = pd.concat([df_pattern, aux2], axis = 0)\n",
    "    \n",
    "    # all details products\n",
    "    df_details = pd.concat([df_details, aux2], axis = 0)\n",
    "    \n",
    "    \n",
    "# reset index    \n",
    "df_details = df_details.reset_index(drop = True)\n",
    "\n",
    "# merge\n",
    "df = df_details.merge(color_info, left_on = 'Art. No.', right_on = 'color_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-employer",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. DATA CLEANING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tropical-drilling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-25T22:19:21.436039Z",
     "start_time": "2022-06-25T22:19:21.075824Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delete $ from records\n",
    "df['price'] = df['price'].apply(lambda x: x.replace('$', '')).str.strip()\n",
    "\n",
    "# composition\n",
    "df = df[~df['Composition'].str.contains('Pocket lining:', na = False)]\n",
    "df = df[~df['Composition'].str.contains('Lining:', na = False)]\n",
    "df = df[~df['Composition'].str.contains('Pocket:', na = False)]\n",
    "\n",
    "# reset index\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "# formats the values of the variable Composition\n",
    "for i in df[df['Composition'].str.contains('Shell:', na = False)]['Composition']:\n",
    "    # index\n",
    "    line = df[df['Composition'] == i].index\n",
    "    \n",
    "    # extract only the compositon\n",
    "    df.loc[line, 'Composition'] = i.split(': ')[1]\n",
    "\n",
    "# change data type - id\n",
    "df['Art. No.'] = df['Art. No.'].astype(int)\n",
    "\n",
    "# change data type - price\n",
    "df['price'] = df['price'].astype(float)\n",
    "\n",
    "# change data type - date\n",
    "df['web_scraping_date'] = pd.to_datetime(df['web_scraping_date'], format = '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# select features\n",
    "df = df[['Art. No.', 'Composition', 'Fit', 'color_url', 'color_name', 'name', 'price', 'web_scraping_date']]\n",
    "\n",
    "# rename \n",
    "df = df.rename(columns = {'Art. No.': 'id', 'Composition': 'composition', 'color_url': 'url', 'color_name': 'color',\n",
    "                          'web_scraping_date': 'date', 'Fit': 'fit'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-recorder",
   "metadata": {},
   "source": [
    "## 3. DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-contractor",
   "metadata": {},
   "source": [
    "### 3.1. Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polar-profit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-25T22:19:21.622143Z",
     "start_time": "2022-06-25T22:19:21.448021Z"
    }
   },
   "outputs": [],
   "source": [
    "query_jeans_schema = \"\"\"\n",
    "                            CREATE TABLE IF NOT EXISTS jeans(\n",
    "                                    id               INTEGER,\n",
    "                                    composition      TEXT,\n",
    "                                    fit              TEXT,              \n",
    "                                    url              TEXT,\n",
    "                                    color            TEXT,\n",
    "                                    name             TEXT,\n",
    "                                    price            REAL,\n",
    "                                    date             TEXT\n",
    "                            );\n",
    "                    \"\"\"\n",
    "\n",
    "# create table\n",
    "conn = sqlite3.connect('../data/database_jeans.sqlite')\n",
    "cursor = conn.execute(query_jeans_schema)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-probability",
   "metadata": {},
   "source": [
    "### 3.2. Insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "specific-count",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-25T22:19:22.443162Z",
     "start_time": "2022-06-25T22:19:21.631152Z"
    }
   },
   "outputs": [],
   "source": [
    "# organize the table\n",
    "data_insert = df[['id', 'date', 'name', 'price', 'fit', 'composition', 'color', 'url']].copy()\n",
    "    \n",
    "# create database connection\n",
    "conn = sqlite3.connect('../data/database_jeans.sqlite')\n",
    "\n",
    "# insert data\n",
    "data_insert.to_sql('jeans', con = conn, if_exists = 'append', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
